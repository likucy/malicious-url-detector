{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce03a9c-79d7-488f-b418-97fc599a0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('E:/作业/杂物/数据集/code_samples.csv')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vec=CountVectorizer()\n",
    "x=data.loc[:,'code']\n",
    "y=data.loc[:,'label']\n",
    "x=vec.fit_transform(x)\n",
    "model=RandomForestClassifier()\n",
    "model.fit(x,y)\n",
    "y_predict=model.predict(x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y,y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4927e5-6209-406d-a0b1-ec5d746959be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5728c7-8be6-48f2-be4d-76f0800a91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 第一步：读取全部数据拟合统一 vectorizer...\n",
      "vectorizer 保存完成！\n",
      " 第二步：分批训练模型...\n",
      "\n",
      " 正在训练第 1 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_1.pkl\n",
      "\n",
      " 正在训练第 2 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_2.pkl\n",
      "\n",
      " 正在训练第 3 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_3.pkl\n",
      "\n",
      " 正在训练第 4 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_4.pkl\n",
      "\n",
      " 正在训练第 5 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_5.pkl\n",
      "\n",
      " 正在训练第 6 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_6.pkl\n",
      "\n",
      " 正在训练第 7 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_7.pkl\n",
      "\n",
      " 正在训练第 8 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_8.pkl\n",
      "\n",
      " 正在训练第 9 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_9.pkl\n",
      "\n",
      " 正在训练第 10 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_10.pkl\n",
      "\n",
      " 正在训练第 11 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_11.pkl\n",
      "\n",
      " 正在训练第 12 批数据（共 20000 条）...\n",
      " 模型已保存：E:/python/models\\rf_model_part_12.pkl\n",
      "\n",
      " 所有模型训练完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import joblib\n",
    "import os  #  加载必要的库\n",
    "\n",
    "data_path = 'E:/作业/杂物/数据集/111_fixed.csv'\n",
    "model_path = 'E:/python/models'\n",
    "chunk_size = 20000\n",
    "trees = 100\n",
    "max_depth = 15  #准备好参数\n",
    "\n",
    "\n",
    "def extract_features(url): #结构化特征，优化使用\n",
    "    features = {} #创建空字典\n",
    "    url = str(url) #确保url都是字符串\n",
    "    features['url_length'] = len(url) # 看长度\n",
    "    features['count_dots'] = url.count('.') #看点的数量\n",
    "    features['count_hyphens'] = url.count('-') #看横杠的数量\n",
    "    suspicious_keywords = ['login', 'secure', 'account', 'verify', 'password', 'update', 'banking'] #看敏感词在恶意网站中的敏感词\n",
    "    features['count_suspicious_words'] = sum( #这里是数组的原因是我们的url不止一个\n",
    "        1 for kw in suspicious_keywords if kw in url.lower()\n",
    "    ) #遍历一遍看有多少敏感词，统计出来个数2\n",
    "    return features\n",
    "\n",
    "if not os.path.exists(model_path): #判断文件是否存在，不存在就创建\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "print(\" 第一步：读取全部数据拟合统一 vectorizer...\")\n",
    "df_all = pd.read_csv(data_path,encoding='ISO-8859-1') #全部数据！！\n",
    "# 清洗：只保留非空的 URL 和标签\n",
    "df_all = df_all.dropna(subset=['url', 'label'])\n",
    "df_all = df_all[df_all['url'].astype(str).str.strip() != '']\n",
    "xx = pd.DataFrame([extract_features(u) for u in df_all['url']])\n",
    "vec = TfidfVectorizer() #创建向量器，类似于片段指向的数字这样的转换过程\n",
    "x_vec=vec.fit_transform(df_all['url'])  #保证训练集每一批是用同一个词袋映射出来的向量，模型每学一批，其实是在同一个特征空间中进化。一致）\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "structured_train_scaled = scaler.fit_transform(xx) #防止数据特征本身太大或太小太影响结果判断转换成介于(0,1)\n",
    "structured_train_sparse = csr_matrix(structured_train_scaled) #转换成稀疏矩阵进行拼接因为前面 x_vec（TF-IDF 特征）就是稀疏矩阵必须跟它格式一样才能拼接！\n",
    "x_final = hstack([x_vec, structured_train_sparse]) #合并数据将特征和数据本身进行合并变成一个\n",
    "y_all = df_all['label']\n",
    "\n",
    "joblib.dump(vec, os.path.join(model_path, 'vectorizer.pkl')) #保存向量器\n",
    "joblib.dump(scaler, os.path.join(model_path, 'scaler.pkl'))\n",
    "print(\"vectorizer 保存完成！\")\n",
    "print(\" 第二步：分批训练模型...\")\n",
    "chunks = pd.read_csv(data_path, chunksize=chunk_size,encoding='ISO-8859-1') #分批读取数据\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n 正在训练第 {i+1} 批数据（共 {len(chunk)} 条）...\")\n",
    "    chunk = chunk.dropna(subset=['url', 'label']) # 先清理掉空数据\n",
    "    x = vec.transform(chunk['url'])  # 用统一的 vectorizer 进行 transform\n",
    "    xx1=pd.DataFrame([extract_features(u) for u in chunk['url']])\n",
    "    y = chunk['label']\n",
    "\n",
    "    structured_scaled2 = scaler.transform(xx1) #不用再fit了！！！我们应当使用之前的规则\n",
    "    structured_sparse2 = csr_matrix(structured_scaled2) \n",
    "    x_final2 = hstack([x, structured_sparse2]) \n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=trees, max_depth=max_depth, random_state=42) #创建模型\n",
    "    rf.fit(x_final2, y) #训练模型\n",
    "    model_filename = os.path.join(model_path, f\"rf_model_part_{i+1}.pkl\") #得到保存路径\n",
    "    joblib.dump(rf, model_filename) #按照上边的路径进行保存\n",
    "    print(f\" 模型已保存：{model_filename}\")\n",
    "print(\"\\n 所有模型训练完成！\") #模型训练完毕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a00d1bc-a240-4950-b1e5-7cff3404ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 加载了 7 个模型\n",
      "[第 1 条] br-icloud.com.br -> 正常 \n",
      "[第 2 条] http://www.garage-pirenne.be/index.php?option=com_content&view=article&id=70&vsig70_0=15 -> 恶意\n",
      "[第 3 条] http://adventure-nicaragua.net/index.php?option=com_mailto&tmpl=component&link=aHR0cDovL2FkdmVudHVyZS1uaWNhcmFndWEubmV0L2luZGV4LnBocD9vcHRpb249Y29tX2NvbnRlbnQmdmlldz1hcnRpY2xlJmlkPTQ3OmFib3V0JmNhdGlkPTM2OmRlbW8tYXJ0aWNsZXMmSXRlbWlkPTU0 -> 正常 \n",
      "[第 4 条] http://www.pashminaonline.com/pure-pashminas -> 正常 \n",
      "[第 5 条] http://www.ikenmijnkunst.nl/index.php/exposities/exposities-2006 -> 正常 \n",
      "[第 6 条] http://www.lebensmittel-ueberwachung.de/index.php/aktuelles.1 -> 正常 \n",
      "[第 7 条] http://www.szabadmunkaero.hu/cimoldal.html?start=12 -> 恶意\n",
      "[第 8 条] http://larcadelcarnevale.com/catalogo/palloncini -> 正常 \n",
      "[第 9 条] http://www.vnic.co/khach-hang.html -> 正常 \n",
      "[第 10 条] signin.eby.de.zukruygxctzmmqi.civpro.co.za -> 正常 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd  # 加载必要的库\n",
    "\n",
    "def extract_features(url): #结构化特征，优化使用\n",
    "    features = {} #创建空字典\n",
    "    url = str(url) #确保url都是字符串\n",
    "    features['url_length'] = len(url) # 看长度\n",
    "    features['count_dots'] = url.count('.') #看点的数量\n",
    "    features['count_hyphens'] = url.count('-') #看横杠的数量\n",
    "    suspicious_keywords = ['login', 'secure', 'account', 'verify', 'password', 'update', 'banking'] #看敏感词在恶意网站中的敏感词\n",
    "    features['count_suspicious_words'] = sum( #这里是数组的原因是我们的url不止一个\n",
    "        1 for kw in suspicious_keywords if kw in url.lower()\n",
    "    ) #遍历一遍看有多少敏感词，统计出来个数2\n",
    "    return features\n",
    "\n",
    "model_path = 'E:/python/models'\n",
    "model_files = sorted([f for f in os.listdir(model_path) if f.endswith('.pkl') and 'rf_model' in f])\n",
    "models = [joblib.load(os.path.join(model_path, f)) for f in model_files]\n",
    "print(f\" 加载了 {len(models)} 个模型\")\n",
    "\n",
    "vec = joblib.load('E:/python/models/vectorizer.pkl')\n",
    "\n",
    "data = pd.read_csv('E:/作业/杂物/数据集/archive/malicious_labeled.csv')\n",
    "urls = data[\"url\"].tolist()[:10]  # 前 10 条做演示\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    sample_vec = vec.transform([url])\n",
    "    \n",
    "    if sample_vec.shape[1] == 0:\n",
    "        print(f\"[第 {idx+1} 条] {url} ->  无法向量化，跳过\")\n",
    "        continue\n",
    "\n",
    "    votes = []\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(sample_vec)[0]\n",
    "        votes.append(pred)\n",
    "    \n",
    "    if not votes:\n",
    "        print(f\"[第 {idx+1} 条] {url} ->  无投票结果\")\n",
    "        continue\n",
    "\n",
    "    final_result = int(np.round(np.mean(votes)))\n",
    "    print(f\"[第 {idx+1} 条] {url} -> {'恶意' if final_result == 1 else '正常 '}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7331119-d9c3-4c01-a99a-11fae3d3ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 12 个模型\n",
      "向量化完成！\n",
      "模型1 正在预测...\n",
      "模型2 正在预测...\n",
      "模型3 正在预测...\n",
      "模型4 正在预测...\n",
      "模型5 正在预测...\n",
      "模型6 正在预测...\n",
      "模型7 正在预测...\n",
      "模型8 正在预测...\n",
      "模型9 正在预测...\n",
      "模型10 正在预测...\n",
      "模型11 正在预测...\n",
      "模型12 正在预测...\n",
      "\n",
      "整体准确率为：0.5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack, csr_matrix #加载必要的库\n",
    "\n",
    "def extract_features(url): #结构化特征，优化使用\n",
    "    features = {} #创建空字典\n",
    "    url = str(url) #确保url都是字符串\n",
    "    features['url_length'] = len(url) # 看长度\n",
    "    features['count_dots'] = url.count('.') #看点的数量\n",
    "    features['count_hyphens'] = url.count('-') #看横杠的数量\n",
    "    suspicious_keywords = ['login', 'secure', 'account', 'verify', 'password', 'update', 'banking'] #看敏感词在恶意网站中的敏感词\n",
    "    features['count_suspicious_words'] = sum( #这里是数组的原因是我们的url不止一个\n",
    "        1 for kw in suspicious_keywords if kw in url.lower()\n",
    "    ) #遍历一遍看有多少敏感词，统计出来个数\n",
    "    return features\n",
    "\n",
    "model_path = 'E:/python/models'\n",
    "vec = joblib.load(os.path.join(model_path, 'vectorizer.pkl')) #加载之前的向量器，可以理解成将文本转换成数字使用的是和之前一样的规则，这个向量器就像规则一样\n",
    "\n",
    "model_files = []\n",
    "for file in os.listdir(model_path):\n",
    "    if file.startswith('rf_model_part_') and file.endswith('.pkl'): #加载所有的模型文件\n",
    "        model_files.append(file)\n",
    "\n",
    "models = []\n",
    "for f in model_files:\n",
    "    models.append(joblib.load(os.path.join(model_path, f))) #加载出所有的模型文件里的模型\n",
    "print(f\"加载了 {len(models)} 个模型\")\n",
    "\n",
    "test_data = pd.read_csv('E:/作业/杂物/数据集/balanced_test_set.csv', encoding='gb18030')  # 加载测试数据集\n",
    "scaler = joblib.load(os.path.join(model_path, 'scaler.pkl')) \n",
    "urls = test_data['url'].tolist() #将数据中的url列单摘出来然后转化成文本\n",
    "true_labels = test_data['label'].tolist() #加载label列\n",
    "xx1 = pd.DataFrame([extract_features(u) for u in urls])\n",
    "x_test = vec.transform(urls)  # 用训练时的 vectorizer.pkl 直接转换\n",
    "print(\"向量化完成！\") #注意这里不能重新fit，只能 transform（防止特征不一致）\n",
    "\n",
    "all_votes = []\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"模型{i+1} 正在预测...\")\n",
    "\n",
    "    structured_scaled2 = scaler.transform(xx1) #不用再fit了！！！我们应当使用之前的规则\n",
    "    structured_sparse2 = csr_matrix(structured_scaled2) #测试集数据也应该这样做\n",
    "    x_final2 = hstack([x_test, structured_sparse2]) \n",
    "\n",
    "    preds = model.predict(x_final2) #用模型进行预测\n",
    "    all_votes.append(preds)  # 维度是 (模型数量, 样本数)因为是多个模型预测每个结果都是一个数组所以每一行是每个模型预测的结果列是模型有几个模型就有几行\n",
    "\n",
    "# 转置一下，得到每个样本的“投票结果”\n",
    "votes_array = np.array(all_votes).T  # 把他变成一行的二维数组\n",
    "# 多数投票：每一行求平均，大于0.5算恶意\n",
    "final_preds = (votes_array.mean(axis=1) >= 0.5).astype(int)\n",
    "acc = accuracy_score(true_labels, final_preds) #计算准确率\n",
    "print(f\"\\n整体准确率为：{acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bc6a2-cbb0-4b80-aa47-8539d8f4c467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
